\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{amsmath}        
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{placeins}

\title{Adversarial Attacks on Capsule Networks}
\bibliographystyle{plain}
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.



\author{%
  Felix Michels, Tobias Uelwer, Stefan Harmeling \\
  Department of Computer Science\\
  Heinrich-Heine University DÃ¼sseldorf\\
  \texttt{\{felix.michels, tobias.uelwer, harmeling\}@hhu.de} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}
\usepackage{amsmath}
\begin{document}

\maketitle

\begin{abstract}
	In this paper we want to extensively evaluate the robustness of capsule networks towards different adversarial attacks. Our experiments show that capsule networks can be fooled as easily as convolutional networks.

\end{abstract}

\section{Introduction}

Adversarial attacks on capsule networks have been previously studied by Marchisio et al. \cite{marchisio}, however they focus on the evaluation of their method. Also Peer et al. \cite{training} have briefly discussed the application of the fast gradient sign method \cite{fgsm} on capsule networks. Detecting adversarial examples using the reconstruction quality of the capsule networks has been investigated by Frosst et al. \cite{darccc}. In this work we want to compare the results of four different attack on capsule networks trained on different datasets and examine the transferability of adversarial perturbations. This paper is structured as followed: in section \ref{lab:capsules} we recapitulate the idea of capsule networks that were introduced by Sabour et al. \cite{capsules}. In section \ref{lab:attacks} we describe the methods we used to attack the capsule networks. Section \ref{lab:experiments} summarizes the results of our experiments.

\section{Capsule Networks and Dynamic Routing}
\label{lab:capsules}
The concept of vector capsules and the dynamic routing algorithm was proposed by Sabour et al. \cite{capsules}. In an essence, neurons are grouped into vectors, so called capsules. Capsules are are meant to represent only a single specific abstract entity, for example a single object class in a classification setting.
In other words, capsules intent to develop a dedicated representation of certain characteristics in a number of dedicated vectors in favor of an entangled representation of many characteristics in a single vector, as it would be the case for MLPs or ConvNets at a given location. This allows to apply linear transformations directly to the representations of respective entities. Spatial relations, which can be implemented as a matrix product, can thus be modeled more efficiently \cite{capsules}.

The routing algorithm introduces a scalar factor, the so called routing coefficient, for each connection between a capsule $i$ from a layer $L$ and capsule $j$ from layer $L+1$. These coefficients explicitly regulate the influence that previous capsules may have on a given capsule $j$ and are determined by the dynamic routing procedure. As this procedure implements a \textit{routing by agreement} mechanism, it yields high coefficients if a capsule $j$ matches the expectation of a capsule $i$ and low coefficients otherwise. [TODO: clarify] Theoretically, that means information flows where it is needed, both during forward and backpropagation. This also supports the goal of capsules with a dedicated representation.

[TODO: Layer (primary, dense?, convcaps)]

[NOTE: text in A.E.]



\section{Adversarial Attacks}
\label{lab:attacks}

Adversarial attacks can be performed in different settings: in the white-box setting the attacker can compute the gradient of the networks output with respect to the input, whereas in the black-box setting such calculations are not possible. Furthermore, adversarial attacks can be classified into targeted attacks, where the goal of the attack is that the network assigns a given label to the manipulated image, and untargeted attacks, where the attacker's goal is to fool the network in the sense that it only missclassifies a given image.

Throughout this paper we denote the input image as $x\in [0,1]^{n\times n}$, the neural network's output before the softmax activation as $F$ and the perturbation as $\delta$. $C(x)$ refers tot the label assigned to $x$ by the network and $C^*(x)$ is the correct label of $x$. Furthermore, we refer to the i-the entry of $F(x)$ as $F(x)_i$.

\subsection{Carlini-Wagner Attack}

The Carlini-Wagner attack \cite{carlini} is a targeted white-box attack and performed by solving the following constrained optimization problem
\begin{equation}
	\begin{aligned}
	& \underset{\delta}{\text{minimize}}
	& & ||\delta||_2 + c \cdot \max(\max_{i\neq t}(F(x+\delta)_i)-F(x)_t, 0) \\
	& \text{subject to}
	& & x+\delta \in [0,1]^{n \times n},
	\end{aligned}
\end{equation}

where $c>0$ is a positive fixed value. To ensure the box-constraint on $x+\delta$ the authors suggested the following transform of variables 
\begin{equation}
	\delta = \frac{1}{2}(\tanh(w)+1)-x,
\end{equation} 
where the $\tanh$-function is applied componentwise. After this transformation the optimization problem is treated as unconstrained and can be solved in terms of $w$ using Adam \cite{adam}. In their work Carlini and Wagner also proposed two different approaches to handle the box-constraint: projected gradient descent and clipped gradient descent. For details we refer the reader to the original work \cite{carlini}.

\subsection{Boundary Attack}

The idea of the boundary attack as it was intoduced by Brendel et al. \cite{boundary} is to sample a perturbation which leads to a missclassification of the original image $x^{(0)}:=x$. Additionally, the desired perturbation should have the smallest possible norm. The initial perturbation $\delta^{(0)}$ is componentwise sampled from a uniform distribution $\delta^{(0)}_{ij}\sim \mathcal{U}(0,1)$. Initial perturbations, which are not missclassified, are rejected. During the attack adversarial images are constructed iteratively $x^{(k+1)}:= x^{(k)}+\delta^{(k)}$ by a random walk close to the decision boundary. During this random walk the following three conditions are enforced by appropriate scaling and clipping of the image and the perturbation:
\begin{enumerate}
	\item The new image $x^{(k+1)}$ should be in the range of a valid image, i.e. in $x^{(k+1)}\in [0,1]^{n\times n}$.
	\item The proportion of the size of the perturbation $\delta^{(k)}$ and the distance to the given image equal to a given hyperparamter $\gamma$.
	\item The reduction of the distance from the adversarial image to the original image $d(x, x^{(k)})-d(x, x^{(k+1)})$ is proportional to $d(x, x^{(k)})$ with constant $\nu>0$.
\end{enumerate}


\subsection{DeepFool Attack}
Deepfool is an untargeted white-box attack developed by Moosavi-Dezfooli eta al \cite{deepfool}.
The authors found, that minimal adversarial perturbations for affine multiclass classifier can be computed exactly and quickly,
by calculated the distance to the (linear) decision boundaries and making an orthogonal projection to the nearest one.
Deepfool initializes $\delta \gets 0$ and then iteratively approximates $F$ with its first degree Taylor polynomial at $x + \delta$, computes a perturbation $\Delta \delta$ for this approximation as described above and updates $\delta \gets \delta + \Delta \delta$.

\subsection{Universal Adversarial Perturbations}
A \emph{universal perturbation} is a single vector $\delta \in \mathbb{R}^{n\times n}$, such that $C(x + \delta) \neq C^*(x)$ for "many" $x$ sampled from the input image distribution.
This concept was proposed by Moosave-Dezfooli et al. \cite{universal} and we use a variation of their algorithm:
\begin{enumerate}
	\item initialize $\delta \gets 0$
	\item choose a batch $X = \{x_1, ..., x_N\}$ of images with $\forall x_i \in\ X:  C(x_i + \delta) = C^*(x_i)$
	\item For each $x_i$ compute a perturbation $\delta_i$ using FGSM \cite{fgsm}
	\item Update the perturbation: $\delta \gets \delta + \frac{1}{N} \sum\limits_{i=0}^N \delta_i$
	\item Clip $\delta$ to a maximal norm chosen beforehand
\end{enumerate}


\section{Experiments}
\label{lab:experiments}

\subsection{Datasets}

We train the capsule network on the following benchmark datasets:
\begin{itemize}
	\item MNIST: $28\times28$ grayscale images of digits (60,000  images for training/10,000  images for testing) \cite{mnist}
	\item Fashion-MNIST:  $28\times28$ grayscale images of fashion items (60,000 images for training/images 10,000 for testing) \cite{fashion}
	\item SVHN: $32\times32$ color images of house numbers (73,257  images for training/26,032  images for testing) \cite{svhn}
	\item CIFAR10: $32\times32$ color images (50.000  images for training/10.000  images for testing) \cite{cifar}
\end{itemize}
Each dataset is divided into ten different classes.

\subsection{Network Architectures}
TODO: Add description of the networks we used. Report test errors.

\begin{table}[h]
	\centering
	\begin{tabular}{lcccc}
		\toprule
		Network       & MNIST & Fashion-MNIST & SVHN & CIFAR10  \\
		\midrule
		Convolutional neural network &  &  &  &  \\
		Capsule network            &  &  &  &  \\
		\bottomrule\\
	\end{tabular}
\label{tab:accuracies}
\caption{Test accuracies achieved by our networks.}
\end{table}

\begin{table}[h]
	\centering
	\begin{tabular}{llcccc}
		\toprule
		Attack & Network       & MNIST & Fashion-MNIST & SVHN & CIFAR10  \\
		\midrule
		\multirow{2}{*}{CW} & CNN &  &  &  &  \\
		& CapsNet            &  &  &  &  \\
		\midrule
		\multirow{2}{*}{Boundary} & CNN &  &  &  &  \\
		& CapsNet            &  &  &  &  \\
		\midrule
		\multirow{2}{*}{DeepFool} & CNN &  &  &  &  \\
		& CapsNet           &  &  &  &  \\
		\midrule
		\multirow{2}{*}{Universal} & CNN &  &  &  &  \\
		& CapsNet           &  &  &  &  \\
		\bottomrule\\
	\end{tabular}
	\label{tab:attacks}
	\caption{Test accuracies on adversarial examples.}
\end{table}

\begin{table}[h]
	\centering
	\begin{tabular}{llcccc}
		\toprule
		Attack & Network       & MNIST & Fashion-MNIST & SVHN & CIFAR10  \\
		\midrule
		\multirow{2}{*}{CW} & CNN &  &  &  &  \\
		& CapsNet            &  &  &  &  \\
		\midrule
		\multirow{2}{*}{Boundary} & CNN &  &  &  &  \\
		& CapsNet            &  &  &  &  \\
		\midrule
		\multirow{2}{*}{DeepFool} & CNN &  &  &  &  \\
		& CapsNet           &  &  &  &  \\
		\midrule
		\multirow{2}{*}{Universal} & CNN &  &  &  &  \\
		& CapsNet           &  &  &  &  \\
		\bottomrule\\
	\end{tabular}
	\label{tab:norms}
	\caption{Average perturbation norm.}
\end{table}


\FloatBarrier
\section{Conclusion}



\bibliography{neurips_2019}


\end{document}
