% !TeX root = ba.tex

\section{Capsule Networks}
\label{sec:capsules}

\subsection{History}

\citet{parse} introduced \emph{credibility networks} to combine image segmentation and recognition by representing images using parse trees.
Later \citet{transformingauto} coined the term \emph{capsule} for substructures of neural network that learn to recognize implicitly defined entities by performing some\textemdash possibly quite complicated\textemdash internal computation and outputting not just the probability of the entity's presence but also a set of \emph{instantiation parameters} encoding the entity's attributes.
The idea was that properly working capsules should have equivariant instantiation parameters but invariant probability that the entity is present with respect to viewpoint changes.

While conventional neural networks form a entangled representation of features in each layer, capsules aim to produce dedicated representations.
CapsNets may utilize these separated representation to improve the information flow to the next layer.
In other words, since the output of each capsule is a vector (or a matrix) and not a scalar like those of units in ordinary neural networks some advanced \emph{routing algorithm} may be employed.

\citet{capsules} first developed such a dynamic routing algorithm, routing-by-agreement, constructed a concrete CapsNet architecture and applied it for the segmentation of highly overlapping digits and other tasks with remarkable results.
Most actual CapsNets use a linear transformation as the internal action for each capsule, but there have been multiple other routing algorithm introduced, most notable EM Routing \citep{em} but also Cognitive Consistency Routing \citep{cognitive}, Dynamic Deep Routing \citep{training} and more.


%% Erics stuff
%The concept of vector capsules and the dynamic routing algorithm was proposed by \citet{capsules}. In an essence, neurons are grouped into vectors, so-called capsules. Each capsule vector is dedicated to a distinct abstract entity, i.e. a single object class in a classification setting. The norm of a capsule vector encodes the probability of the represented object being present in the input, while the vector orientation encodes the object's characteristics. Thus, CapsNets aim to develop dedicated representations that are distributed into multiple vectors in contrast to convolutional networks that utilize an entangled representation in a single vector at a given location. This allows the application of linear transformations directly to the representations of respective entities. Spatial relations, which can be implemented as a matrix product, can thus be modeled more efficiently.
%
%CapsNets are organized in layers. Initially, the original CapsNet applies a convolutional layer. The resulting feature maps are then processed by the primary capsule layer. Internally, it applies a series of convolutional layers on its own, each yielding a spatial grid of capsules. Within all capsule layers the \emph{squashing} function serves as a vector-to-vector non-linearity that squashes each capsule vector length between $0$ and $1$ while leaving the orientation unaltered. Subsequently, convolutional or densely connected capsule layers can be applied. While the latter does not utilize weight sharing, convolutional capsule layers share the kernels over the spatial grid, as well as capsules from the previous layer. These layers effectively estimate output capsules based on respective input capsules. The dynamic routing algorithm determines each agreement between estimate and iteratively calculated output capsule. This is done by introducing a scalar factor, the so-called routing coefficient, for each connection between an estimate and respective output. Such an output is defined as the sum over all respective estimates, weighted by their routing coefficients. Theoretically, that means information flows where it is needed, both during forward and backpropagation. This non-parametric procedure supports the goal of capsules with clean dedicated representations. To improve results, an additional capsule may be used within the routing algorithm to serve as a dead end for information that may not be linked to known abstract capsule categories. This is also referred to as the \emph{none-of-the-above} category.
%% End erics stuff

Given that routing-by-agreement is still the most commonly used method we will exclusively consider this approach in this paper.

\subsection{Routing-By-Agreement}

\newcommand{\uhat}{\hat{u}_{j \vert i}}

A capsule in the routing-by-agreement setting consists of a vector in the unit ball.
Its direction encodes the instantiation parameters and its euclidean length the probability that the attached entity is present.

For a capsule $i$ of dimension $d_1$ let us designate the output with $u_i \in \Set{ \mathbb{R}^{d_1} | \norm{u} \leq 1}$.
If $i$ is connected to a capsule $j$ of the next layer with dimension $d_2$ the network includes a learned transformation matrix $W_{ij} \in \mathbb{R}^{d_1 \times d_2}$.
Thus we can compute $\uhat = W_{ij} u_i$.
The vector $\uhat$ can be seen as a prediction of the feature's parameters in capsule $j$ given the information of the feature in capsule $i$.
The output for the capsule $j$ then is a linear combination with dynamically computed coupling coefficients of these predictions over all connected capsules in the previous layer as described in Algorithm~\ref{alg:routing}.

The routing in this way is differentiable and can be trained using backpropagation.
The $b_{ij}$ used in Algorithm~\ref{alg:routing} are logits for the coupling coefficients, therefore their initial value can be seen as a bias for the routing algorithm and can be trained. This seems to offer little benefit, so we initialize them as zero (as done by \cite{capsules}).

Like in conventional neural networks there are multiple types of layers depending on the possible connections to the units in the lower layer. If each capsule in a layer is connected with every capsule in the layer before it, we call it a \emph{dense capsule layer}.
Like in a convolutional layer, capsule can also be arranged in a spatial grid (in the shape of $height \times width \times types \times dimension$) where each capsule is only connected to those in it's receptive field and capsules of the same type share weights.
Such a configuration is called a \emph{convolutional capsule layer}.
Of course such layers, and the routing used between them, are only applicable if two capsule layers are placed consecutively.
To convert convert the output of scalar layers to capsules we use \emph{primary capsule layers}.
These layers either apply a fully connected or a convolutional layer to their input, reshape the result into capsules and squashes them so that their vector length is between zero and one.

A typical CapsNet for image recognition begins with one or more convolutional layers, followed by a primary convolutional layer, optionally some number of convolutional capsule layers and finally a dense capsule layer with one capsule per class.
I a fully trained CapsNet those class capsules should contain all instantiation parameters necessary to recreate the input image.
Thus a \emph{reconstruction network} that takes the capsule of the class capsule with largest norm as input and tries to reconstruct the input image can be trained simultaneously.
The euclidean distance between the original image and reconstruction, the \emph{reconstruction loss}, may be used as a regularization technique.
Since images also often contain some unneeded information that cannot be sensibly linked to a capsule in the next layer, an additional capsule may used to serve as a dead end, which is referred to as the \emph{none-of-the-above} category.

\algtext*{EndFor} % Remove end for text
\algtext*{EndProcedure}
\begin{algorithm}
\caption[Routing-by-agreement]{Routing-by-agreement as proposed by \citet{capsules}}
\label{alg:routing}
\begin{algorithmic}[1]
	
\Procedure{Routing}{$\hat{u}, r$}
	\State $\forall i,j: b_{ij} \gets 0$
	\For{$r$ iterations}
		\State $c_{ij} \gets \frac{\exp(b_{ij})}{\sum_{k}{\exp(b_{ik})}}$ \Comment{Softmax}
		\State $s_j \gets \sum_{i}{c_{ij}\uhat}$
		\State $v_j \gets \frac{\norm{s_j}}{1 + \norm{s_j}^2} s_j$ \Comment{Squash capsules}
		\State $b_{ij} \gets b_{ij} + \langle v_j, \uhat \rangle$	
	\EndFor
	\Return{$v$}
\EndProcedure
	
\end{algorithmic}
\end{algorithm}