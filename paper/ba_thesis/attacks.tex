% !TeX root = ba.tex

\section{Adversarial Attacks}
\label{sec:attacks}

In general, adversarial attacks describe any techniques with the purpose to disturb the performance of machine learning models.
While adversarial attacks exist for a wide variety of applications like speech recognition or natural language processing, this work will concentrate on adversarial attacks on image classification tasks.
\emph{Adversarial examples} refer to modified images which are very close to some original image, but are classified differently by the model.
In our context they are in particular only mentioned in relationship to the original image.
The \emph{adversarial perturbation} is the difference between the adversarial example and the original.

There are various attack scenarios differing in the knowledge, goals and abilities of the attacker.
In \emph{white-box} attacks, all information about the model architecture, training images and learned parameters is available to the attacker, while in the \emph{black-box} scenario only the networks output can be observed.
In particular, gradients with respect to the input can be computed in the white-box setting.
These terms are in general not strictly defined and many intermediate stages exist, including limited access to the models output, information about the architecture but not the learned parameters or knowledge about the training images but not the architecture etc.

\emph{Targeted} and \emph{untargeted} describes the objective of the attacker.
With an untargeted attack an adversarial example is just classified as any other label than the true label, while an adversarial example from a targeted attack has to be classified as a given target label chosen by the attacker beforehand.
Targeted attacks are strictly more difficult for the attacker, but also more useful.
Furthermore, in some instances untargeted adversarial examples are easy to generate, or rather are near impossible to defend against, just because some classes may be conceptually very close to each other.
For example, differentiating between a Golden Retriever and a Labrador Retriever in ImageNet \citep{imagenet} can even be quite challenging for humans.

The last type of adversarial attacks that we will test are \emph{universal perturbations}.
Most attacks compute a different adversarial perturbation for each input image, but a universal perturbation aims to fool many images while using the same perturbation.
Non-universal perturbations are easier to find, but universal perturbations are simpler for the attacker to apply in a realistic attack scenario.
Additionally, the mere existence of universal perturbations reveals significant flaws in the used architecture, more so than that of normal adversarial examples.

An important problem to consider is the definition of \emph{closeness} to the original image. Possibilities for such an image similarity metric include simple $L_p$ norms (including $p=0$),
or more sophisticated metrics, like entropy based metrics \citep{umbrella}.
While no metric exists which reliable simulates human perception of image similarity, and $L_p$ norms may be insufficient in this regard \citep{lpnorms}, they still give us information about the robustness of the neural networks.
Here we will examine adversarial perturbations optimized towards a small $L_2$ norm, since it is often used in discussion about adversarial attacks and is a good compromise between the other commonly used, but more specific scenario targeting $L_0$ and $L_\infty$ norms.

Throughout this work $x \in [0,1]^n$ will denote the (flattened) input image and $\delta \in [0,1]^n$ the perturbation.
The label assigned to $x$ by the neural network will be referred to as $C(x)$, while the correct label is $C^*(x)$.\\
The networks output as logits is $Z(X)$ and the output interpretable as probabilities is $F(x)$.
This means, in the case of the ConvNet we have $F(x) = \mathrm{softmax}(Z(x))$ and in the case of the CapsNet $Z(x) = \mathrm{arctanh}(2F(x) - 1)$. \\
Furthermore, any gradient in the following section refers to the gradient with respect to the input image $x$ and not the network's parameters.

\subsection{Carlini-Wagner Attack}

\citet{carlini} extensively evaluated a wide range of adversarial attacks.
In particular they developed the following targeted white-box attack which will be referred to as the Carlini-Wagner (CW) attack.

Starting with the general problem for targeted attacks for a given label t
\begin{equation}
\begin{aligned}
& \minimize_{\delta} && \norm{\delta} \\
& \text{subject to} && C(x + \delta) = t \\
& && x + \delta \in [0,1]^n .
\end{aligned}
\end{equation}
Using Lagrangian relaxation, this is transformed to the following problem
\begin{equation}
\label{eq:cwproblem}
\begin{aligned}
& \minimize_{\delta} && \norm{\delta} + c \cdot f(x + \delta)\\
& \text{subject to} && x + \delta \in [0,1]^n ,
\end{aligned}
\end{equation}
where $c > 0$ is a suitable chosen constant and $f$ is an \emph{objective function}, i.e.\ a function with the property, that
$f(x + \delta) \leq 0$ if and only if $C(x + \delta) = t$.
There are many possible objective functions, but a particularly good choice is
\begin{equation}
\label{eq:cwobjective}
f(x') = \max \Set{ \max \Set{Z(x')_i | i \neq t } - Z(x')_t, -\kappa } .
\end{equation}%

The \emph{confidence parameter} $\kappa$ determines with how much confidence the adversarial example is classified as the target class, i.e.\ how close to the decision boundary it is.
The optimal value for $c$ in \Cref{eq:cwproblem} is the smallest value, that results in an adversarial example and can be found using a binary search.

This box-constrained problem can further be simplified by introducing the variable $w \in \mathbb{R}^n$ and setting $\delta_i = \frac{1}{2} (\tanh(w_i) + 1)$.
This substitution yields an unconstrained problem, which can be solved with various popular methods. We used the Adam optimizer \citep{adam}.

The Carlini-Wagner attack is generally a quite strong attack and leads to almost undetectable adversarial examples.
In particular, it can find adversarial examples even when defensive techniques like distillation are utilized \citep{carlini}.
However, due to the binary search it can often be rather slow.

\subsection{Boundary Attack}

The boundary attack is a black-box attack proposed by \citet{boundary}. It is furthermore a \emph{decision based} attack. This means, not only is no knowledge of the model architecture or the learned weights required, but also the output scores of the network are hidden from the attack. Only the final decision $C(x)$ can be used to construct an adversarial example.
Unlike other attacks, the boundary attack does not start with the original image and modifies it, but instead starts with a (possible random) misclassified image and changes it to resemble the original image.

The boundary attack iteratively generates adversarial examples $x^{(k)}$ (and therefore perturbations $\delta^{(k)} = x^{(k)} - x$) using a random walk. At the beginning it samples $x^{(0)} \sim \mathcal{U}(0,1)^n$ until $C(x^{(0)}) \neq C^*(x)$, i.e.\ $x^{(0)}$ is an adversarial example for $x$.
The next iteration should then fulfill the following criteria to ensure that the sequence $(x^{(k)})_{k \in \mathbb{N}}$ has a limit point on the decision boundary:

\begin{enumerate}
	\item The new image $x^{(k+1)}$ is in the range of a valid image:
	\begin{equation*}
	x^{(k+1)}\in [0,1]^n
	\end{equation*}
	
	\item The size of the random step is proportional to the size of the perturbation $\delta^{(k)}$ for a parameter $\gamma > 0$:
	\begin{equation*}
	\norm{\delta^{(k+1)} - \delta^{(k)}} = \gamma \cdot \norm{\delta^{(k)}}
	\end{equation*}
	
	\item The distance to the original is reduced with a factor $0<\nu<1$:
	\begin{equation*}
	\norm{\delta^{(k+1)}} = \nu \cdot \norm{\delta^{(k)}}
	\end{equation*}
\end{enumerate}

While these conditions are difficult to meet exactly, they are approximated by sampling from a normal distribution orthogonal to $\delta^{(k)}$ and making a step with size $\gamma$ in this direction.
If this point is still adversarial, a step towards $x$ is made with size $\nu$.
The parameters $\gamma$ and $\nu$ are adapted dynamically, similarly to Trust Region \citep{trustregion} methods.
A moving average of the success rate of the orthogonal of the direct step is kept and the parameters are increased or decrease, if this average deviates too much from beforehand chosen optimal value.

The success rate for the orthogonal step should be close to $0.5$, the idea being that if $x^{(k)}$ is close to the decision boundary, which is locally linear almost everywhere for most types of neural networks, then approximately $50\%$ of random orthogonal steps should pass the decision boundary. 
The target success rate for the direct step is set to a value around $0.25$, which empirically leads to good adversarial examples.
The algorithm terminates once $\nu$ is close to zero.

\subsection{DeepFool Attack}

\citet{deepfool} developed the untargeted white-box attack \emph{DeepFool}.
The authors calculate minimal adversarial perturbations for a linear classifier by projecting the original image to the nearest decision boundary.
By approximating the network with its first order Taylor polynomial, these calculations can be applied to the nonlinear classifier.
In detail, the perturbation is initialized as $\delta^{(0)} = 0$.
For each class label $i \neq C^*(x)$ the distance to the decision boundary is estimated with

\begin{equation}
%\begin{aligned}
l_i = 
\frac{
	\abs{Z_i(x + \delta^{(k)}) - Z_{C^*(x)}(x + \delta^{(k)})}
}
{
	\norm{\nabla Z_i(x + \delta^{(k)}) - \nabla Z_{C^*(x)}(x + \delta^{(k)})}
}
%\end{aligned}
\end{equation}

\todo{How is this similar to Carlini-Wagner?}

and $\delta^{(k+1)}$ is the projection to the nearest boundary under the distance approximation from above.
Our version of the DeepFool attack slightly differs from the original version in insofar that we clip $\delta^{(k)}$ to obtain $x + \delta^{(k)} \in [0,1]^n$, and that we restrict $\norm{\delta^{(k)}}$ at each step, which yields better adversarial examples in our experience.

\subsection{Universal Adversarial Perturbations}

The concept of universal adversarial perturbations was proposed by \citet{universal} and refers to a single perturbation vector $\delta \in \mathbb{R}^n$ such that $C(x + \delta) \neq C^*(x)$ for many different $x$ sampled from the input image distribution.
To do this, we use following variation of their algorithm:

As long as the accuracy on the test set is above a previously chosen threshold, repeat these steps:
\begin{enumerate}
	\item Initialize $\delta^{(0)} \gets 0$.
	\item Sample a batch $X^{(k)} = \{x_1^{(k)}, ..., x_N^{(k)}\}$ of images with $\forall x \in\ X^{(k)}:  C(x + \delta^{(k)}) = C^*(x)$.
	\item For each $x_i^{(k)}$ compute a perturbation $\delta_i^{(k+1)}$ using FGSM \citep{fgsm}.
	\item Update the perturbation: $\delta^{(k+1)} \gets \delta^{(k)} + \frac{1}{N} \sum\limits_{i=0}^N \delta_i^{(k+1)}$
\end{enumerate}

This differs insofar from the original algorithm described by \citet{universal} that in step 3 approximate perturbations for a whole batch is computed, while the computed an optimal perturbation for each $x_i$ using their previous work DeepFool.

In principle many other attacks instead of FGSM can be used, however we found that for our applications FGSM reached adequate results compared to much slower methods.
Since FGSM uses the gradient of the network, FGSM and therefore this algorithm for computing universal adversarial perturbations, are white-box attacks.
