% !TeX root = ba.tex

\newcommand{\norm}[1]{\left \lVert #1 \right \rVert}

\section{Adversarial Attacks}

\todo{Explain potential dangers due to attacks}
In general adversarial attacks describe any techniques, that cause machine learning models to perform poorly.
While adversarial attacks exist for a wide variety of applications like speech recognition or natural language processing, this work will concentrate on adversarial attacks on image classification tasks.
In particular \emph{adversarial examples} refer to modified images which are very close to some original image, but are classified differently by the model.
The \emph{adversarial perturbation} is the difference between the adversarial example and the original.
- Different attack scenarios:
white-box / black-box \\
targeted / untargeted \\
specific / universal \\
An important problem to consider is the definition of \emph{close} to the original image. Possibilities for such an image similarity metric include simple $L_p$ norms (including $p=0$),
or more sophisticated metrics, like entropy based metrics \todo{Find that paper with the umbrellas}.
While no metric exists which reliable simulates human perception of image similarity, and $L_p$ norms may be insufficient in this regard \cite{lpnorms}, they still give us information about the robustness of the neural networks. Here we will exclusively examine adversarial perturbations optimized towards a small $L_p$ norm \todo{explain why}.

Throughout this work $x \in [0,1]^n$ will denote the input image $\delta \in [0,1]^n$ the perturbation.
The label assigned to $x$ by the neural network will be referred to as $C(x)$, while the correct label is $C^*(x)$.
$Z(X)$ is the networks output as logits and $F(x)$ is the output interpretable as probability.
This means, in the case of the ConvNet we have $F(x) = \mathrm{softmax}(Z(x))$ and in the case of the CapsNet $Z(x) = \mathrm{arctanh}(2F(x) - 1)$.



\subsection{Carlini-Wagner Attack}

Carlini and Wagner extensively evaluated a wide range of adversarial attacks \cite{carlini}.
In particular they developed the following white-box, targeted attack, that will be referred to as the Carlini-Wagner (CW) attack.

Starting with the general problem for targeted attacks

\begin{equation}
\begin{aligned}
& \minimize_{\delta} && \norm{\delta} \\
& \text{subject to} && C(x + \delta) = t \\
& && x + \delta \in [0,1]^n
\end{aligned}
\end{equation}

Using Lagrangian relaxation, this is transformed to

\begin{equation}
\begin{aligned}
& \minimize_{\delta} && \norm{\delta} + c \cdot f(x + \delta)\\
& \text{subject to} && x + \delta \in [0,1]^n
\end{aligned}
\end{equation}

where $c > 0$ is a suitable chosen constant and $f$ is an \emph{objective function}, i.e. a function with the property, that
$f(x + \delta) \leq 0$ if and only if $C(x + \delta) = t$.
There are many possible objective functions, but a particularly good choice is
$f(x') = \max \{ \max \{Z(x')_i : i \neq t \} - Z(x')_t, -\kappa \}$.
\todo{Sounds weird, rewrite this}
The \emph{confidence parameter} $\kappa$ determines, how close to the decision boundary the adversarial example is.
The optimal value for $c$ is the smallest value, that results in an adversarial example and is found using a binary search.

This box-constrained problem can further be simplified by introducing the variable $w \in \mathbb{R}^n$ and setting $\delta_i = \frac{1}{2} (\tanh(w_i) + 1)$.
This substitution yields an unconstrained problem, which can be solved with various popular methods. We used the Adam (\cite{adam}) optimizer.

The Carlini-Wagner attack is generally a quite strong attack and leads to almost undetectable adversarial examples.
\todo{citation}
In particular, it can find adversarial examples even when defensive techniques like distillation are utilized \cite{carlini}.
However, due to the binary search it can often be quite slow.

\subsection{Boundary Attack}

The boundary attack is a black-box attack proposed by Brendel et al. \cite{boundary}. It is furthermore a \emph{decision based} attack. This means, not only is no knowledge of the model architecture or the learned weights required, but also the output scores of the network are hidden from the attack. Only the final decision $C(x)$ can be used to construct an adversarial example.
Unlike other attacks, the boundary attack does not start with the original image and modifies it, but instead starts with a (possible random) misclassified image and changes it to resemble the original.

\todo{write the rest}
Random walk, bla bla

\subsection{DeepFool Attack}
Moosavi-Dezfooli et al. developed the untargeted white-box attack \emph{DeepFool} \cite{deepfool}.
The authors calculated minimal adversarial perturbations for linear classifier, by projecting the original image to the nearest decision boundary.

\subsection{Universal Adversarial Perturbations}
The concept of universal adversarial perturbations was proposed by Moosavi-Dezfooli et al. \cite{universal} and refers to a single perturbation vector $\delta \in \mathbb{R}^n$, such that $C(x + \delta) \neq C^*(x)$ for many different $x$ sampled from the input image distribution.