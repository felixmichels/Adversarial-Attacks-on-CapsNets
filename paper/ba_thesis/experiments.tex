% !TeX root = ba.tex

\section{Experiments}
\label{sec:experiments}

\subsection{Datasets and Network Architectures}

We trained models on each of the following benchmark data sets:

\begin{itemize}
	\item MNIST: $28\times28$ grayscale images of handwritten digits (60,000  images for training / 10,000  images for testing) \cite{mnist}
	\item Fashion-MNIST:  $28\times28$ grayscale images of fashion items (60,000 images for training / images 10,000 for testing) \cite{fashion}
	\item SVHN: $32\times32$ color images of house numbers (73,257  images for training / 26,032 images for testing) \cite{svhn}
	\item CIFAR-10: $32\times32$ color images of animals and vehicles (50.000  images for training / 10.000  images for testing) \cite{cifar}
\end{itemize}

Each of the four data sets consist of ten classes.

As baseline architectures we trained convolutional networks utilizing max-pooling,
batch normalization \citep{batchnorm} and dropout \citep{dropout}.
Training capsule networks can be comparatively more difficult in practice, therefore we had to carefully construct well-suited architectures:

For the MNIST data set we used three layer CapsNet like that of \citet{capsules}, but with only 64 convolutional kernels in the first layer. \\
For the Fashion-MNIST and the SVHN data sets we used CapsNets with two convolutional layers in the beginning, followed by the primary capsule layer and the class capsules. \\
Simple CapsNets however don't perform very well on more complex data like CIFAR-10 \citep{complex}, therefore we use a modified DCNet \citep{dcnet} with three convolutional capsule layers and a non-of-the-above category for the dynamic routing. \\
We train all CapsNets using margin loss and the reconstruction loss for regularization \citep{capsules}, and we use three iterations in the dynamic routing. \\
All CapsNets as well as ConvNets are trained with the Adam optimizer \citep{adam}. \\
For more details on the model architectures, please refer to the tables in Appendix~\ref{lab:networks}.

The test accuracies of our networks on the respective data sets are displayed in Table~\ref{tab:accuracies}.
While these accuracies do not reach the state of the art, the similarity between the performances of the CapsNets and ConvNets indicates their suitability to the task of comparing adversarial robustness.

\begin{table}
	
	%\vskip 0.15in
	\centering%\scalebox{0.85}{
	\begin{tabular}{lcccc}
		\toprule
		Network       & MNIST & Fashion-MNIST & SVHN & CIFAR-10  \\
		\midrule
		ConvNet           & $99.39\%$ & $92.90\%$ & $92.57\%$ & $88.22\%$ \\
		CapsNet           & $99.40\%$ & $92.65\%$ & $92.35\%$ & $88.21\%$ \\
		\bottomrule\\
	\end{tabular}%}
	\caption{Test accuracies achieved by our networks.}
	\label{tab:accuracies}
\end{table}

For each of the data sets we compute adversarial examples using our four chose attacks in the following manner: \\
\todo{Make this less horrible}
We randomly select $1000$ each from the test set and attack them with DeepFool and the boundary attack.
For Carlini-Wagner (with hyperparameter $\kappa=1$) we calculate $500$ adversarial examples for randomly chosen images from the test set and randomly chosen target labels (but that are different from the true labels). In the case of the universal perturbation we divide the test set into ten parts and compute on each part ten adversarial perturbations.

We do not restrict the maximal perturbation norm for any of the attacks and hence the Carlini-Wagner, the boundary and the DeepFool attack only generate valid adversarial examples.
Regarding the universal perturbations, we terminate the algorithm once accuracy falls below $50\%$.
We found, that the universal perturbations generalize well: while only a tenth of the test set is used to generate each universal perturbation, they can reduce the accuracy on the whole test set to $50\pm2.5\%$.

In Figure~\ref{tab:images} some examples of the product of the attacks on CIFAR-10 can be seen. The Carlini-Wagner, the boundary and the DeepFool attack result in humanly imperceptible adversarial examples. Only the universal perturbations are clearly visible. Furthermore we can observe some differences in the structure of the adversarial perturbations. While the black-box attack (boundary attacks) produces perturbations that resemble random noise, some noticeable patterns emerge in the perturbations of the white-box attacks. For more examples of adversarial images for this and the other data sets, see Appendix~\ref{lab:images}.

\begin{center}
\begin{longtable*}{rccc}
	
	CW & \includegraphics[height=2cm]{cifar10_carlini_wagner_orig.pdf} & \includegraphics[height=2cm]{cifar10_carlini_wagner_caps.pdf} & \includegraphics[height=2cm]{cifar10_carlini_wagner_conv.pdf}\\
	\\
	Boundary & \includegraphics[height=2cm]{cifar10_boundary_attack_orig.pdf} & \includegraphics[height=2cm]{cifar10_boundary_attack_caps.pdf} & \includegraphics[height=2cm]{cifar10_boundary_attack_conv.pdf}\\
	\\
	DeepFool & \includegraphics[height=2cm]{cifar10_deepfool_orig.pdf} & \includegraphics[height=2cm]{cifar10_deepfool_caps.pdf} & \includegraphics[height=2cm]{cifar10_deepfool_conv.pdf}\\
	\\
	Universal & \includegraphics[height=2cm]{cifar10_universal_perturbation_orig.pdf} & \includegraphics[height=2cm]{cifar10_universal_perturbation_caps.pdf} & \includegraphics[height=2cm]{cifar10_universal_perturbation_conv.pdf}\\
	\\
\end{longtable*}
% This should count as a figure and not as a table
%\addtocounter{table}{-1}
\captionof{figure}[Adversarial Examples on CIFAR-10]{Original images from the CIFAR-10 dataset (left), adversarial examples and perturbations for CapsNet (middle) and adversarial examples and perturbations for ConvNet (right). Pixel values of perturbation images are scaled for visibility.}
\label{tab:images}
\end{center}

\subsection{Robustness to Adversarial Attacks}

The most import aspect with respect to the dangers of adversarial attacks is how much an image has to be modified for an successful attack (i.e.\ the size of the perturbation in some metric. In our case this is the euclidean norm, see Section~\ref{sec:attacks}).
These results are displayed in Table~\ref{tab:norms}. On the data sets except MNSIST the CapsNets is more susceptible to the Carlini-Wagner attack than the ConvNets.

We observe the same trend for the adversarial examples calculated using the boundary attack. This means, that CapsNets are in general neither more robust against white-box attacks nor against black-box attacks than ConvNets, as long as the attacks are sufficiently strong.
Against DeepFool however the CapsNets perform slightly better and have significantly higher adversarial perturbation norms at all data sets except CIFAR-10.

The biggest difference between the architectures is the behavior against universal perturbations, where the norms of the perturbations for the CapsNets are considerably higher for the MNIST, Fashion-MNIST and SVHN data sets. Especially the perturbations on SVHN calculated for the CapsNet are not just clearly humanly visible, the original image is in some cases not recognizable anymore (see Figure~\ref{fig:universal-img}).
Only the universal perturbations on CIFAR-10 have similar norms for the CapsNet and the ConvNet.

However we have to mention, that while the existence of adversarial examples with small norms shows the lack of robustness, proving resistance to adversarial attacks is vastly more difficult.
After all, adversarial perturbations with large norms can just be the result of an inadequate attack.

\begin{table}
	\centering\
	\begin{tabular}{llcccc}
		\toprule
		Attack & Network       & MNIST & Fashion & SVHN & CIFAR-10  \\
		\midrule
		\multirow{2}{*}{CW} & ConvNet & {$1.40$} & $0.51$ & $0.67$ & $0.37$ \\
		& CapsNet            & $1.82$ & {$0.50$} & {$0.60$} & {$0.23$} \\
		\midrule
		\multirow{2}{*}{Boundary} & ConvNet & {$3.07$} & $1.24$ & $2.42$ & $1.38$ \\
		& CapsNet            & $3.26$ & {$0.93$} & {$1.88$} & {$0.72$} \\
		\midrule
		\multirow{2}{*}{DeepFool} & ConvNet & {$1.07$} & {$0.31$} & {$0.41$} & $0.23$ \\
		& CapsNet           & $2.02$ & $0.55$ & $0.80$ & {$0.16$} \\
		\midrule
		\multirow{2}{*}{Universal} & ConvNet & {$6.71$} & {$2.61$} & {$2.46$} & {$2.45$} \\
		& CapsNet           & $11.45$ & $5.31$ & $8.59$ & $2.70$ \\
		\bottomrule\\
	\end{tabular}
	\caption[Average Perturbation Norms]{Average perturbation norms for each attack and architecture.}
	\label{tab:norms}
\end{table}

\citet{transfer} demonstrated the existence of \emph{cross-technique transferability} for a wide variety of machine learning techniques. This means, that adversarial examples constructed for a specific classifier may also be able to fool another classifier using a vastly different machine learning model. Such transfer attacks can be seen as black-box attacks where the first model functions as a oracle.

To measure the transferability we evaluate the fooling rate of the adversarial examples calculated for CapsNets when applied to the ConvNets and vice versa (see Table~\ref{tab:transfer}). We define an image to \emph{fool} a network in the case of the untargeted attacks (boundary, DeepFool, universal) if it is misclassified and in the case of targeted attacks (Carlini-Wagner) if it is classified as the target label for that the adversarial example was computed. \todo{Include graphic?}

The transfer fooling rates for the Carlini-Wagner attack are quite small, even though we used a non-zero confidence parameter of $\kappa=1$. The untargeted boundary and DeepFool attacks have higher fooling rates in general and match the result of Table~\ref{tab:norms} for most data sets, as in adversarial examples with high norms lead to a higher transfer fooling rate. This is not the case for the boundary attack with MNIST and the DeepFool attack with Fashion-MNIST. However the difference in fooling rates for the latter case is quite small and it is not surprising that MNIST would produce an outlier, since the adversarial perturbations for this data set are so large. \todo{This doesn't really make sense this way}


The most noteworthy finding here are the fooling rates for the universal perturbations. Although the norms of the CapsNet universal perturbations were much larger (see Table~\ref{tab:norms}), they perform poorly on the ConvNet, while the small perturbations computed for the ConvNet achieve relatively high fooling rates on the CapsNet. Especially for SVHN the universal perturbation of the ConvNet is much more effective on the CapsNet than the perturbations computed on the CapsNet itself.

		
\begin{table}
	\centering
	\begin{tabular}{llcccc}
		\toprule
		Attack & Network       & MNIST & Fashion & SVHN & CIFAR-10  \\
		\midrule
		\multirow{2}{*}{CW} & ConvNet & $0.8\%$ & $1.2\%$ & $2.8\%$ & $2.4\%$ \\
		& CapsNet            & $2.0\%$ & $2.0\%$ & $3.8\%$ & $2.0\%$ \\
		\midrule
		\multirow{2}{*}{Boundary} & ConvNet & $8.8\%$ & $9.5\%$ & $10.5\%$ & $13.4\%$ \\
		& CapsNet            & $14.2\%$ & $14.6\%$ & $12.9\%$ & $26.1\%$ \\
		\midrule
		\multirow{2}{*}{DeepFool} & ConvNet & $4.3\%$ & $8.5\%$ & $13.5\%$ & $11.8\%$ \\
		& CapsNet           & $0.9\%$ & $10.9\%$ & $10.8\%$ & $14.1\%$ \\
		\midrule
		\multirow{2}{*}{Universal} & ConvNet & $4.9\%$ & $20.4\%$ & $35.0\%$ & $25.9\%$ \\
		& CapsNet           & $38.2\%$ & $25.7\%$ & $53.4\%$ & $47.2\%$ \\
		\bottomrule\\
	\end{tabular}
	\caption[Transfer Fooling Rates]{Fooling rates of adversarial examples calculated for a CapsNet and evaluated on a ConvNet and vice versa. For the universal attack we report the accuracy on the whole test set.}
	\label{tab:transfer}
\end{table}

\subsection{Structural Analysis of Adversarial Examples}

While we have found, that CapsNets are not necessarily more robust against adversarial attack, the low success rate of the transfer attacks suggest, that there may be some structural difference between adversarial examples computed for ConvNets and those computed for CapsNets.

\subsubsection{Universal Attack t-SNE}

We use t-SNE \citep{tsne} to visualize the normalized universal perturbations created for the CapsNet and the ConvNet (see Figure~\ref{fig:tsne}).
We discover, that for all data sets there is a clear distinction between the perturbations calculated for the different architectures.
Although t-SNE can often distinguish the perturbations of models that are trained slightly different, this is not just a outcome of this phenomenon.
By including another ConvNet in the diagram we find the clusters of the ConvNets almost perfectly matching while the cluster for the CapsNet is still separated.
This means, that the universal perturbation of CapsNets and ConvNets are intrinsically different.

In Figure~\ref{fig:tsne} can furthermore subclusters based on the data set splits used to calculate the perturbations be seen.
These distinctions are however less pronounced with the other data sets besides CIFAR-10.

\begin{figure}
	\includegraphics{tsne.pdf}
	\caption[t-SNE Plot of Universal Perturbations]{Two dimensional embedding of the universal perturbations on CIFAR-10 calculated using t-SNE \citep{tsne}. The upper right cluster represents perturbations computed on a ConvNet, whereas the cluster in the lower left represents those calculated on a CapsNet. Perturbations with the same color were created using the same subset of test data.}
	\label{fig:tsne}
\end{figure}


\subsubsection{SVD}
\citet{universal} considered singular values of the matrix containing normalized adversarial examples to determine, if adversarial examples lie in a low dimensional subspace. \\
For this purpose, let us denote with $\delta(x)$ the minimal adversarial perturbation for the input $x \in [0,1]^n$,
and $ N = \begin{bmatrix}
\frac{\delta(x_1)} {\norm{\delta(x_1)}},  ...,  \frac{\delta(x_k)} {\norm{\delta(x_k)}} 
\end{bmatrix}
\in \mathrm{R}^{n \times k}
$ for some $x_1, ..., x_k$ in the test set with $k > n$. \\
$\delta(x)$ is orthogonal to the decision boundary, assuming it is reasonably smooth, therefore the singular values of $N$ give us information about the decision boundary. For example, for an binary linear classifier, $N$ would have a rank of one, i.e.\ only one non-zero singular value.

Accurately computing $\delta(x)$ is very challenging, so we used the DeepFool attack as an approximation. We computed an additional $5000$ adversarial examples and plot the size of the singular values of the resulting Matrix $N$ together with a matrix containing columns sampled from the unit sphere $S^{n-1}$ in Figure~\ref{fig:svd}.

For both the CapsNets and the ConvNets the singular values decay much more quickly than those of the random matrix, confirming the findings of \citet{universal}.
In this regard the difference between CapsNets and ConvNets is inconclusive.
Whilst the curve of the CapsNet decreases more slowly than that of the ConvNet in the case of the CIFAR-10 data set, the opposite occurs for the other cases.


\begin{figure}
	\centering
	\begin{subfigure}{\textwidth}
		\includegraphics[width=.95\textwidth]{svd_cifar10.pdf}
		\caption{CIFAR-10}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\includegraphics[width=.95\textwidth]{svd_mnist.pdf}
		\caption{MNIST}
	\end{subfigure}
	
	\caption{Singular values of the matrix containing normal vectors to the decision boundary.}
	\label{fig:svd}
\end{figure}

\subsection{Effects on Activations}
What am I even doing here.

$d(z, w) = \frac{\norm[\infty]{z - w}}{\norm[\infty]{z}}$

\begin{figure}
	\centering
	\begin{subfigure}{\textwidth}
		\includegraphics{activation_error_caps.pdf}
		\caption{CapsNet. Dashed lines are capsule norms}
	\end{subfigure}
		\begin{subfigure}{\textwidth}
		\includegraphics{activation_error_conv.pdf}
		\caption{ConvNet}
	\end{subfigure}

	\caption[Activation errors]{Activation errors of select layers}
	\label{fig:activation}
\end{figure}